{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this DataLab, you will revisit one of the MLP models you developed in the previous DataLab and dive deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#random image\n",
    "index = np.random.randint(0, 60000)\n",
    "plt.imshow(X_train[index])\n",
    "print(y_train[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (10000, 28, 28), (60000,), (10000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.1**\n",
    "\n",
    "Adjust the shapes of X_train, X_test, y_train, y_test to be compatible with the network below.\n",
    "\n",
    "<img src=https://i.imgur.com/OFNAslJ.png width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(60000, 784)\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_test = X_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.2**\n",
    "\n",
    "Build the model. But this time, create a function for this task because you will have to repeat it a few times today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 30)                23550     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 30)                930       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                310       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,790\n",
      "Trainable params: 24,790\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "def build_model(print_summary=False):\n",
    "    model = Sequential()\n",
    "    # YOUR CODE HERE #\n",
    "    model.add(Dense(30, input_shape=(784,), activation='relu'))\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))    \n",
    "    \n",
    "    # display summary is optional\n",
    "    if print_summary:\n",
    "        model.summary()\n",
    "    return model\n",
    "\n",
    "model = build_model(print_summary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.3**\n",
    "\n",
    "Compile the model. Similarly, create a function for this task. In this DataLab we will use the Adam optimizer.\n",
    "\n",
    "During compilation, we can define an optimizer as `model.compile(optimizer='adam', ...)`\n",
    "\n",
    "This is a good starting point, but it does not let us configure the optimizer. Most importantly, when we want to change the learning rate. \n",
    "\n",
    "To achieve this, you can do the following:\n",
    "\n",
    "`from keras.optimizers import Adam\n",
    "opt = Adam(lr=0.001)\n",
    "model.compile(optimizer=opt, ...)`\n",
    "\n",
    "now this will let you change the learning rate.\n",
    "\n",
    "Note:\n",
    "\n",
    "If importing `Adam` gives an error try the following instead:\n",
    "\n",
    "`from tensorflow.keras.optimizers import Adam`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "def compile_model(model, lr):\n",
    "    # YOUR CODE HERE #\n",
    "    opt=Adam(lr=lr)\n",
    "    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = compile_model(model, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.4**\n",
    "\n",
    "Train the model for 5 epochs. This time save the history `H = model.fit(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.5**\n",
    "\n",
    "Evaluate the model using `model.evaluate()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = model.evaluate(X_test, y_test)\n",
    "print(val_loss, val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.6**\n",
    "\n",
    "Plot the learning curves for `loss` and `val_loss`. Example output:\n",
    "\n",
    "<img src=https://i.imgur.com/74WymKX.png width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_plotter(H):\n",
    "    plt.plot(H.history['loss'], label='Training Loss')\n",
    "    plt.plot(H.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "loss_plotter(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.7**\n",
    "\n",
    "Plot the learning curves for `accuracy` and `val_accuracy`. Example output:\n",
    "\n",
    "<img src=https://i.imgur.com/Md9JIHh.png width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_plotter(H):\n",
    "    plt.plot(H.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(H.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "accuracy_plotter(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the functions you created in task 1.\n",
    "model = build_model()\n",
    "model = compile_model(model, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.1**\n",
    "\n",
    "Now instead of setting a fixed number of epochs, use `EarlyStopping` callback. Monitor the validation loss with a patience of 2. Remember to restore the best weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** When using early stopping, you still need to set the number of epochs for training. Let's say you set the epochs to 10, and the model ran for 10 epochs. This means the stopping conditions you set in the callback did not occur therefore the training ran until the end. If the training runs for less than 10 epochs, this means early stopping kicked in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.2**\n",
    "\n",
    "Train the model with the callback you created in Task 2.1. From now on, always store the history `H = model.fit()`. Set the number of epochs such that you can observe early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=30, batch_size=32, callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.3**\n",
    "\n",
    "Plot the learning curves. Interpret your results.\n",
    "\n",
    "- Compare these to the learning curves from Task 1\n",
    "- Did the model train for more than 5 epochs?\n",
    "- Did the performance improve?\n",
    "- Do you think the model is overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plotter(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_plotter(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Tuning learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your model with 5 different learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [1e-2, 1e-3, 1e-4, 1e-5, 1e-6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the early stopping callback you created in Task 2. When you are training the models, save each history object. Set the number of epochs to a large enough number such that early stopping occurs in some of the learning rates.\n",
    "\n",
    "**Task 3.1** Plot `val_loss` vs epochs for each learning rate.\n",
    "\n",
    "Example output:\n",
    "\n",
    "<img src=https://i.imgur.com/CPqfa3j.png width=\"500\">\n",
    "\n",
    "closer look:\n",
    "\n",
    "<img src=https://i.imgur.com/BvSZA0s.png width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots show which learning rate(s) are a good choice. But we can plot the best loss vs learning rate to see the relationship even better. \n",
    "\n",
    "**Task 3.2** Plot best validation loss vs learning rate.\n",
    "\n",
    "Example output:\n",
    "\n",
    "<img src=https://i.imgur.com/xSgQbyZ.png width=\"500\">\n",
    "\n",
    "Notice that we can use the validation loss obtained at the end of training (last epoch). But that is not necessarily the best. In fact, we are using early stopping for this reason: when the model is not improving, stop with a patience. If you configured the callback correctly, it returns the best model. \n",
    "- You can use `model.evaluate()` to calculate the best validation loss.\n",
    "- Alternatively, since patience is 2, the 3<sup>rd</sup> validation loss from the last must be the best loss in the history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = [] # for plotting val_loss vs epochs for each learning rate\n",
    "best_val_losses = [] # for plotting best_val_loss vs learning rate\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print('Training with: ', lr)\n",
    "    model = build_model()\n",
    "    model = compile_model(model, lr=lr)\n",
    "    # YOUR CODE HERE #\n",
    "    earlystop = EarlyStopping(monitor='val_loss', patience=2)\n",
    "    \n",
    "    # Train the model with early stopping #\n",
    "    H = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=30, batch_size=32, callbacks=[earlystop])\n",
    "    \n",
    "    best_loss = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    best_val_losses.append(best_loss)\n",
    "    histories.append(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 3.1 #\n",
    "# Plot validation loss vs. epochs for each learning rate #\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    plt.plot(histories[i].history['val_loss'], label='lr:'+str(lr))\n",
    "plt.title('Validation Loss vs. Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 3.2 #\n",
    "# Plot best validation loss vs learning rate # \n",
    "\n",
    "plt.plot(range(len(learning_rates)), best_val_losses, 'o-')\n",
    "plt.xticks(range(len(learning_rates)), learning_rates)\n",
    "plt.title('Best Validation Loss vs. Learning Rate')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Best Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Creative Brief\n",
    "\n",
    "Apply what you have learned to your creative brief.\n",
    "\n",
    "- Plot learning curves\n",
    "- Use early stopping\n",
    "- Train your model with different learning rates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See creative brief "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
