{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block C: Responsible AI \n",
    "\n",
    "For details regarding ILO 3.1 and the use-case, please refer to the Assessment rubric in Microsoft Teams, and the [DataLab: Responsible AI](https://adsai.buas.nl/Study%20Content/Responsible%20and%20Explainable%20AI/UseCases.html) GitHub page."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Use-case 1: Identifying, and describing bias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define the concept of bias in relation to the Imsitu dataset, and a computer vision task."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The systematic inaccuracy or unfairness that is introduced into the data or the algorithm used to carry out the task is referred to as bias in the context of the Imsitu dataset and a computer vision task. As a result, various groups or people may not be represented fairly or treated equally in the data or task outcomes. When certain preconceived conceptions, assumptions, or prejudices have an impact on how specific agents or entities are labeled in the Imsitu dataset, bias might result."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### List, and describe the type of bias that you identified in the Imsitu dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I discovered bias in the Imsitu dataset when I searched for the verb \"praying\" and categorized the agent. One agent was labeled as a \"woman,\" based on an accompanying picture of a hand praying. However, there is no clear indication that the hand belongs to a woman, as the presence of nail polish does not necessarily mean the individual is female, as both men and women can wear it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discuss the possible ramification (e.g., harm) in terms of fairness of the identified bias instance: Why, and when, is this particular instance of bias undesirable? In other words, who might be disproportionally affected by this particular instance of bias, and when does this negative effect come into play?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The identified bias in the Imsitu dataset can harm fairness and lead to unequal treatment when used in computer vision tasks. The incorrect labeling of the agent as a \"woman\" based on the presence of nail polish reinforces gender stereotypes and can result in incorrect predictions by computer vision systems, causing harm to individuals who don't conform to these stereotypes. Addressing bias in datasets and algorithms is crucial to ensure fair and equitable outcomes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use-case 2: Propose individual fairness method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify a sensitive/protected attribute in the Imsitu dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Religion (praying)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mitigate bias in the Imsitu dataset by applying the ‘Fairness Through Unawareness' or ‘Fairness Through Awareness' method to this sensitive/protected attribute."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Fairness Through Awareness\" strategy entails including the sensitive/protected characteristic in the model and making sure that the model is trained to make fair decisions, independent of this feature. Different religious beliefs often have unique practices and rituals, including the way they offer prayers. By incorporating these distinct religious practices into a system or organization, the types of prayers can be made more distinguishable.Furthermore, by recognizing and acknowledging the diversity of religious beliefs, it reduces the risk of offense caused by grouping certain images or symbols together."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elaborate on the individual fairness method that you applied, and why you think it is a good method to mitigate bias in the Imsitu dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I explained this in the previous question."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use-case 3: Create a subset of images from the original dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training set that contains images from the selected classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cardboard found\n",
      "n14799601\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# load imsitu_space.json file\n",
    "imsitu_space = json.load(open(\"data/imsitu_space.json\"))\n",
    "\n",
    "nouns = imsitu_space[\"nouns\"]\n",
    "verbs = imsitu_space[\"verbs\"]\n",
    "\n",
    "# function to get all agent codes for a specific agent/noun\n",
    "def get_agent_codes(agent = \"person\"):\n",
    "    for noun in nouns:\n",
    "        if nouns[noun]['gloss'][0] == agent:\n",
    "            print(f\"{agent} found\")\n",
    "            print(noun)\n",
    "\n",
    "# get all agent codes for cardboard\n",
    "get_agent_codes(\"cardboard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['stapling_141.jpg',\n",
       "  'stapling_199.jpg',\n",
       "  'stapling_70.jpg',\n",
       "  'stapling_138.jpg',\n",
       "  'stapling_75.jpg'],\n",
       " ['stapling', 'stapling', 'stapling', 'stapling', 'stapling'],\n",
       " ['item', 'item', 'item', 'item', 'item'],\n",
       " ['n14799601', 'n14799601', 'n14799601', 'n14799601', 'n14799601'],\n",
       " 5)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#function to get all verb codes for a specific verb\n",
    "def get_verb_agent(json_file, verb_custom, agent_custom):\n",
    "    train = json.load(open(json_file))\n",
    "    verb_value = []\n",
    "    agent_key = []\n",
    "    agent_value = []\n",
    "    file_path = []\n",
    "    count = 0\n",
    "    for i in train:\n",
    "        verb = train[i]['verb']\n",
    "        if verb == verb_custom:\n",
    "            frames = train[i]['frames']\n",
    "            for frame in frames:\n",
    "                for key, value in frame.items():\n",
    "                    if key == 'item':\n",
    "                        if value in agent_custom:\n",
    "                            if i not in file_path:\n",
    "                                agent_key.append(key)\n",
    "                                agent_value.append(value)\n",
    "                                file_path.append(i)\n",
    "                                verb_value.append(verb)\n",
    "                                count += 1\n",
    "                        else:\n",
    "                            continue\n",
    "                    else:\n",
    "                        continue\n",
    "    return(file_path, verb_value, agent_key, agent_value, count)\n",
    "\n",
    "get_verb_agent('data/train.json', 'stapling', ['n14799601'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/original/stapling_141.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18504\\2106440936.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_folder\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestination_folder\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mimg_to_folder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/original/\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"data/cardboard/train/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18504\\2106440936.py\u001b[0m in \u001b[0;36mimg_to_folder\u001b[1;34m(dirs_original, dirs_destination)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimage_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msource_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestination_folder\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdirs_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             \u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_folder\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestination_folder\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mimg_to_folder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/original/\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"data/cardboard/train/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\sterr\\anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    425\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[0mdst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m     \u001b[0mcopyfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m     \u001b[0mcopymode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\sterr\\anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    262\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/original/stapling_141.jpg'"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "def img_to_folder(dirs_original, dirs_destination):\n",
    "    image_list = get_verb_agent('data/train.json', 'stapling', ['n14799601'])[0]\n",
    "    dirs_list = [(dirs_original, dirs_destination)]\n",
    "    for img in image_list:\n",
    "        for source_folder, destination_folder in dirs_list:\n",
    "            shutil.copy(source_folder+img, destination_folder+img)\n",
    "\n",
    "img_to_folder(\"data/original/\", \"data/cardboard/train/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def lists_to_df(dirs_destination, col1_name, col2_name, col3_name):\n",
    "    col1 = get_verb_agent('data/json/train.json', 'stapling', ['n14799601'])[0]\n",
    "    col2 = get_verb_agent('data/json/train.json', 'stapling', ['n14799601'])[1]\n",
    "    col3 = get_verb_agent('data/json/train.json', 'stapling', ['n14799601'])[3]\n",
    "    df = pd.DataFrame(list(zip(col1, col2, col3)), columns=[col1_name, col2_name, col3_name])\n",
    "    df.to_csv(dirs_destination, index=False)\n",
    "    return df\n",
    "\n",
    "lists_to_df('./data/dusting/train/stapling_train.csv', 'file_name','verb', 'agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib\n",
    "import itertools\n",
    "import random, os, glob\n",
    "from imutils import paths\n",
    "from sklearn.utils import shuffle\n",
    "from urllib.request import urlopen\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import othet dataset\n",
    "dir_path = \"data/dataset-resized\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = (224, 224)\n",
    "waste_labels = {\"cardboard\":0, \"glass\":1, \"metal\":2, \"paper\":3, \"plastic\":4, \"trash\":5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "  x = []\n",
    "  labels = []\n",
    "  image_paths = sorted(list(paths.list_images(path)))\n",
    "  for image_path in image_paths:\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, target_size)\n",
    "    x.append(img)\n",
    "    label = image_path.split(os.path.sep)[-2]\n",
    "    labels.append(waste_labels[label])\n",
    "  x, labels = shuffle(x, labels, random_state=42)\n",
    "  input_shape = (np.array(x[0]).shape[1], np.array(x[0]).shape[1], 3)\n",
    "  print(\"X shape: \", np.array(x).shape)\n",
    "  print(f\"Number of Labels: {len(np.unique(labels))} , Number of Observation: {len(labels)}\")\n",
    "  print(\"Input Shape: \", input_shape)\n",
    "  return x, labels, input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (2527, 224, 224, 3)\n",
      "Number of Labels: 6 , Number of Observation: 2527\n",
      "Input Shape:  (224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "x, labels, input_shape = load_dataset(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18504\\3178595693.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Split the data into training and validation sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mImages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Images' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(Images, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use-case 4: Write Python functions; group fairness metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your text for use-case 4 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your Python code for use-case 4 here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use-case 5: Write Python function; group fairness taxonomy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your text for use-case 5 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your Python code for use-case 5 here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use-case 6: Apply one/multiple explainable AI method(s) to the image classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your text for use-case 6 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your Python code for use-case 6 here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df0198c0c923084a692d9dd8837021393278b07223290056eb4ec625bff0d58b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
